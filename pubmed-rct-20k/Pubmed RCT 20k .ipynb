{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922ddbd-5a4c-4e14-94ad-e078e5d86817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Importcustom modules\n",
    "import data_loader\n",
    "import eda_plots\n",
    "import text_processing\n",
    "\n",
    "# 2. Load Data\n",
    "train, test = data_loader.load_pubmed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f603037-b0b2-4a9e-9405-25e63c1f516d",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Integrity & Preprocessing Decisions\n",
    "### Handling Pre-Normalized Numbers (@)\n",
    "* **Observation**: The dataset creators have pre-normalized all integers and floats to the @ symbol.\n",
    "* **Implication**: This symbol carries semantic weight. For example, \"efficacy of @ weeks\" provides context that \"efficacy of weeks\" lacks.\n",
    "* **Preprocessing Strateg**y: When cleaning the text (removing punctuation), we must preserve the @ symbol. Removing it risks losing the placeholder for dosage, time, or statistical significance.\n",
    "\n",
    "### Missing Values\n",
    "* **Observation**: The dataset contains 0 missing values.\n",
    "* **Insight**: This indicates a highly curated dataset, which is rare in real-world scenarios. No imputation is required.\n",
    "\n",
    "## 2. Target Variable Analysis & Evaluation\n",
    "### Label Distribution\n",
    "* **Consistency**: The distribution of labels between Train and Test sets is almost identical (e.g., Methods constitutes ~33% in both). This ensures that our evaluation metrics will be reliable and representative of generalization performance.\n",
    "* **Class Imbalance**: The model will likely be biased toward the majority classes: \"Methods\" and \"Results\" (statistically the \"safest\" bets).\n",
    "* **Evaluation Strategy**: We cannot rely solely on Accuracy. We must monitor Precision, Recall, and F1-score to ensure the model correctly identifies minority classes like Objective or Background.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ab022-5f04-4a23-a9b1-48c0fb7bdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run EDA (Visualizations)\n",
    "eda_plots.plot_positional_bias(train)\n",
    "eda_plots.plot_label_distribution(train, title='Train Label Dist')\n",
    "eda_plots.plot_sentence_lengths(train)\n",
    "eda_plots.plot_top_words(train)\n",
    "eda_plots.plot_transition_matrix(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebdf16b-fcef-41a8-80a7-02e8c4b8592f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Structural Features: Length & Position\n",
    "### Sentence Length & Padding\n",
    "* **Outliers**: The \"Max\" sentence length (296 tokens) is an extreme outlier. The vast majority of sentences are much shorter.\n",
    "* **Deep Learning Strategy**: Padding every sequence to 296 is a waste of memory (sparse data). A length of 50–60 tokens covers >95% of the data. Truncating to this range will significantly speed up training with minimal information loss.\n",
    "\n",
    "### Positional Embeddings (line_number)\n",
    "* **Observation**: The sentence_id (0, 1, 2...) is highly predictive.\n",
    "    * **Sentence 0** → Almost always Background or Objective.\n",
    "    * **Sentence 10+** → Almost always Results or Conclusion.\n",
    "* **Insight**: The classification of a sentence depends heavily on where it appears in the abstract.\n",
    "\n",
    "## 4. Linguistic Analysis: Vocabulary & Syntax\n",
    "### A. The \"Comparator\" Vocabulary (Stopwords)\n",
    "* **High Frequency**: Words like \"between\", \"with\", \"than\", and \"compared\" are dominant.\n",
    "* **Insight**: These are Relational Words defining the experiment (e.g., \"Better than placebo\"). Removing these as \"stopwords\" destroys the directionality of the result. We must keep them for the deep learning models.\n",
    "\n",
    "### B. Punctuation Signals\n",
    "* **Semicolons (;)**: Uniquely high frequency (Rank 24). In academic writing, semicolons separate distinct thoughts or contrasting sentiments within a single sentence (e.g., \"Pain decreased*;** however, nausea increased\"*). A sequence model (RNN/Transformer) can leverage this to detect shifts in context.\n",
    "\n",
    "### C. Hyphenated Concepts\n",
    "* **Medical Glue**: Hyphens are essential in medicine (e.g., IL-6, TNF-alpha, Double-blind).\n",
    "* **Tokenization**: If we remove hyphens, Double-blind becomes Double blind. This is acceptable as a Bigram model will still capture the relationship.\n",
    "\n",
    "### D. Domain-Specific Fingerprints\n",
    "* **Trial Design (Predicts METHODS)**: randomized, control, intervention, placebo.\n",
    "* **Outcome (Predicts RESULTS)**: p-value, significant, CI, mean, efficacy. Note: Efficacy (ideal conditions) is distinct from Effectiveness (real world).\n",
    "* **Subjects & Dosage**: patients (sick) vs participants (healthy). mg is the only top-50 unit, suggesting a dominance of Pharmacological Trials.\n",
    "* **Artifacts**: rsb/lsb (brackets) often wrap trial registration numbers at the very end of abstracts, potentially acting as accidental predictors for Conclusions.\n",
    "\n",
    "## 5. N-Gram Analysis (Contextual Signals)\n",
    "### The \"Statistical Signature\"\n",
    "* **Key Bigrams**: ('p', '@'), ('statistically', 'significant').\n",
    "* **Insight**: The strongest signal for Results isn't just \"p\", but \"p\" followed by a number.\n",
    "* **Action**: When using TF-IDF, we must set ngram_range=(1, 2) to capture these pairs.\n",
    "\n",
    "### The \"Unit\" Glue\n",
    "* **Context Dependency**: The symbol @ is ambiguous on its own.\n",
    "    * @ + mg = Dosage\n",
    "    * @ + weeks = Duration\n",
    "    * @ + patients = Sample Size\n",
    "* **DL Preview**: An LSTM/RNN will look at the next word to resolve the ambiguity of the @ symbol.\n",
    "\n",
    "### Ranges & Safety\n",
    "* **Ranges**: The #1 bigram is ('@', '@'), representing ranges like \"10-20 mg\".\n",
    "* **Safety**: The bigram ('adverse', 'events') is a strong signal for side effects, usually appearing in Results/Conclusions.\n",
    "\n",
    "## 6. Sequential Dependencies (Transition Matrix)\n",
    "Our analysis of label transitions reveals that medical abstracts follow a strict narrative structure:\n",
    "1. **Linear Flow**: Abstracts never move backward (e.g., Results → Background is impossible).\n",
    "2. **The \"Sticky\"** Sections: Methods and Results have high self-loops (multiple sentences in a row).\n",
    "3. **The \"Bridge\"**: Objective is rarely multi-sentence. It acts as a quick transition from Background to Methods.\n",
    "\n",
    "## 7. EDA Summary\n",
    "Our exploration confirms that this dataset is not just a Bag-of-Words, but a **structured narrative**. Successful modeling requires three layers of features:\n",
    "1. **Vocabulary**: Retention of statistical markers (p, @, CI) and relational stopwords.\n",
    "2. **Position**: Leveraging line_number (Sentence 0 vs Sentence 10).\n",
    "3. **Sequence**: Exploiting the predictable flow: Background → Objective → Methods → Results → Conclusions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7043448-1cc2-480c-9902-da22b1eec154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data Processing (Cleaning & Tokenization)\n",
    "# Apply cleaning to Train\n",
    "train = text_processing.clean_text_initial(train)\n",
    "train = text_processing.tokenize_and_lemmatize(train)\n",
    "\n",
    "# Apply cleaning to Test\n",
    "test = text_processing.clean_text_initial(test)\n",
    "test = text_processing.tokenize_and_lemmatize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70789885-0b90-4a96-ac0d-d76035c2492d",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Text Preprocessing Pipeline\n",
    "### Step 1: Cleaning & Tokenization\n",
    "* **Normalization**: We apply lowercasing and standard punctuation removal.\n",
    "* **The @ Symbol**: Crucially, we preserve the @ symbol. As noted in the EDA, this is a placeholder for numbers. Removing it destroys semantic context (e.g., \"efficacy of weeks\" vs \"efficacy of @ weeks\").\n",
    "* **Safety Check**: We run a pass to remove any residual integers, ensuring that the only numerical signal remaining is the standardized @ placeholder.\n",
    "Whitespace: Redundant whitespace is collapsed to ensure clean tokenization.\n",
    "\n",
    "### Step 2: Custom Stopword Removal\n",
    "* **Noise Removal**: Based on our frequency plots, we remove dataset artifacts like rsb and lsb (bracket codes).\n",
    "* **Domain Preservation**: We **keep** high-frequency domain words (randomized, p, patient, mg) as they are strong predictors.\n",
    "* **Relational Restoration**: We deliberately **restore** relational words (between, against, with). In medical texts, \"Difference **between** groups\" carries meaning that \"Difference groups\" does not.\n",
    "\n",
    "### Step 3: Lemmatization\n",
    "* **Technique**: We apply WordNetLemmatizer.\n",
    "* **Goal**: To map grammatical variations to their root (e.g., studies → study, analyzed → analyze). This reduces vocabulary size and prevents the model from treating \"study\" and \"studies\" as two unrelated features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299b59b-dffa-4a7f-b0c7-a6ce2dbbf917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Get Model Parameters (Vocabulary size & Sequence Length)\n",
    "max_features, max_length = text_processing.get_corpus_stats(train)\n",
    "\n",
    "# 6. Encode Labels\n",
    "label_encoder, train_y, test_y = text_processing.encode_labels(train, test)\n",
    "\n",
    "# Now you are ready for ML/DL models!\n",
    "print(\"\\nReady for Model Training.\")\n",
    "print(train[['processed_text_final', 'label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2f293-5b44-4a55-826d-a76405ea9bda",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Vectorization Strategy\n",
    "\n",
    "### The \"95% Rule\" (Input Dimensions)\n",
    "Instead of guessing max_features or max_length, we derived them statistically from the training set:\n",
    "* **Vocabulary**: A size of **12,843** words covers 95% of all token occurrences.\n",
    "* **Sequence Length**: 95% of sentences contain **30 tokens** or fewer.\n",
    "* **Decision**: We use these exact values for our model inputs. This ensures computational efficiency while retaining the vast majority of information.\n",
    "\n",
    "### Feature Extraction Comparison\n",
    "We compare two distinct approaches to representing medical text:\n",
    "* **Sparse (TF-IDF)**: Captures precise keywords and bigrams (e.g., p < @, double blind).\n",
    "* **Dense (Word2Vec)**: We trained a custom Word2Vec model to generate semantic embeddings. Sentence vectors are created by averaging the word vectors.\n",
    "\n",
    "### Standardization & The \"Sparsity\" Trap\n",
    "* **Word2Vec (Standardized)**: We applied StandardScaler. Since dense vectors have varying ranges, scaling ensures linear models (SVM, LogReg) treat all dimensions equally and converge faster.\n",
    "* **TF-IDF (Not Standardized)**: We **skipped** standardization for TF-IDF.\n",
    "    * **Reason**: TF-IDF produces a **sparse matrix** (mostly zeros). Standardization requires \"centering\" the data (subtracting the mean), which turns every zero into a non-zero number.\n",
    "    * **Consequence**: This would destroy sparsity, causing memory usage to explode and crashing the kernel. Additionally, TF-IDF is already naturally normalized between 0 and 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd9d02-f79d-48a0-a99e-7be5c8e0e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Feature Extraction\n",
    "X_train_bow, X_test_bow, _ = feature_extraction.get_bow_features(\n",
    "    train['processed_text_final'], test['processed_text_final'], \n",
    "    max_features_suggested, custom_stop_words\n",
    ")\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, _ = feature_extraction.get_tfidf_features(\n",
    "    train['processed_text_final'], test['processed_text_final'], \n",
    "    max_features_suggested, custom_stop_words\n",
    ")\n",
    "\n",
    "X_train_w2v, X_test_w2v = feature_extraction.get_word2vec_features(\n",
    "    train['final_tokens'], test['final_tokens']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d7515-9c26-4651-9a45-0abe367bb739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train & Eval Classical Models\n",
    "feature_sets = {\n",
    "    \"Bag-of-Words\": (X_train_bow, X_test_bow),\n",
    "    \"TF-IDF\": (X_train_tfidf, X_test_tfidf),\n",
    "    \"Word2Vec\": (X_train_w2v, X_test_w2v)\n",
    "}\n",
    "\n",
    "ml_results = ml_models.train_evaluate_ml_models(\n",
    "    feature_sets, train_y_enc, test_y_enc, label_encoder.classes_\n",
    ")\n",
    "print(\"\\nTop Classical Models:\")\n",
    "print(ml_results.sort_values(by=\"F1-Score (Weighted)\", ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e5c7d-4f31-438c-8cfd-30a8fde516e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Machine Learning Baselines\n",
    "### Modeling Strategy\n",
    "We established a \"score to beat\" using classical algorithms: **Naive Bayes** (speed), **Logistic Regression** (baseline), **Linear SVM** (linear separation), and **Random Forest** (ensemble).\n",
    "* **Metric Choice**: Due to class imbalance (few \"Objectives\", many \"Methods\"), we prioritized F1-Weighted, Precision, and Recall over simple Accuracy.\n",
    "\n",
    "### Results & The \"Ceiling\"\n",
    "* **Winner: Logistic Regression with TF-IDF (~77.3% Accuracy)**.\n",
    "* **Insight**: For this specific task, precise keywords (\"randomized\", \"p-value\") are more predictive than averaged semantic embeddings (Word2Vec ~71%). Averaging word vectors \"blurs\" the distinct signals required to separate sections.\n",
    "* **The Problem**: All ML models hit a ceiling around 77%. The confusion matrices reveal they consistently confuse **Background** with **Objective**. Without knowing the sequence (i.e., Background comes before Objective), classical models cannot solve this ambiguity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f3be7-b3a3-45dc-a116-0aac46ffbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEP LEARNING PREP\n",
    "# 1. One-Hot Labels\n",
    "train_y_oh, test_y_oh = dl_utils.encode_labels_one_hot(train_y_enc, test_y_enc)\n",
    "\n",
    "# 2. Text Vectorization\n",
    "vectorizer = dl_utils.create_text_vectorizer(\n",
    "    train['processed_text_final'], max_features_suggested, max_length_suggested\n",
    ")\n",
    "\n",
    "train_seq = vectorizer(train['processed_text_final']).numpy()\n",
    "test_seq = vectorizer(test['processed_text_final']).numpy()\n",
    "\n",
    "# 3. Positional Features (For Hybrid)\n",
    "train_line_oh, train_total_oh = dl_utils.prepare_positional_features(train, \"train\")\n",
    "test_line_oh, test_total_oh = dl_utils.prepare_positional_features(test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826a835-7cfb-4544-bb8b-a0e087405f3c",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Deep Learning Transition\n",
    "### Data Preparation\n",
    "* **Labels**: We use OneHotEncoder to convert integers (0, 1, 2) into binary class vectors for the neural network.\n",
    "* **TextVectorization**: This is the bridge between human language and deep learning math. Unlike TF-IDF (which loses order), this layer converts sentences into sequences of integers, preserving the exact order of words.\n",
    "\n",
    "### Experiment A: Standard LSTM\n",
    "* **Architecture**: We built a Standard LSTM (Long Short-Term Memory) network. Unlike ML models that \"average\" text, the LSTM processes inputs sequentially, understanding context (e.g., negation).\n",
    "* **Result**: **~80.0%** Accuracy. It beat the ML baseline, but still struggled with the \"Background vs. Objective\" distinction.\n",
    "\n",
    "### Experiment B: Bidirectional LSTM\n",
    "* **Hypothesis**: Reading the sentence backwards and forwards might capture more context.\n",
    "* **Result**: Negligible improvement (**~0.1% gain**) with higher training time.\n",
    "* **Key Insight**: The bottleneck is **not** textual understanding. The model understands what is being said, but it lacks the context of where it is being said.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c666e-2127-4366-849e-bd48297ebe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEP LEARNING MODELS\n",
    "VOCAB_SIZE = len(vectorizer.get_vocabulary())\n",
    "EMBED_DIM = 128\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "dl_results_list = []\n",
    "\n",
    "# --- Model A: LSTM ---\n",
    "print(\"\\nTraining LSTM...\")\n",
    "lstm_model = dl_models.build_lstm(VOCAB_SIZE, EMBED_DIM)\n",
    "hist_lstm = lstm_model.fit(\n",
    "    train_seq, train_y_oh, epochs=5, batch_size=64,\n",
    "    validation_data=(test_seq, test_y_oh), callbacks=[early_stop], verbose=1\n",
    ")\n",
    "dl_utils.plot_history(hist_lstm, \"LSTM\")\n",
    "# Evaluate\n",
    "pred_lstm = tf.argmax(lstm_model.predict(test_seq), axis=1)\n",
    "dl_results_list.append(dl_utils.calculate_dl_results(test_y_enc, pred_lstm, \"LSTM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03359a51-573d-4b99-a6d4-44112c407c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model B: Bi-LSTM ---\n",
    "print(\"\\nTraining Bi-LSTM...\")\n",
    "bilstm_model = dl_models.build_bilstm(VOCAB_SIZE, EMBED_DIM)\n",
    "hist_bilstm = bilstm_model.fit(\n",
    "    train_seq, train_y_oh, epochs=5, batch_size=64,\n",
    "    validation_data=(test_seq, test_y_oh), callbacks=[early_stop], verbose=1\n",
    ")\n",
    "dl_utils.plot_history(hist_bilstm, \"Bi-LSTM\")\n",
    "# Evaluate\n",
    "pred_bilstm = tf.argmax(bilstm_model.predict(test_seq), axis=1)\n",
    "dl_results_list.append(dl_utils.calculate_dl_results(test_y_enc, pred_bilstm, \"Bi-LSTM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdccdbc7-67ae-48c9-8e29-710dc697a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model C: Hybrid (Tribrid) ---\n",
    "print(\"\\nTraining Hybrid Model...\")\n",
    "hybrid_model = dl_models.build_hybrid_model(VOCAB_SIZE, EMBED_DIM, max_length_suggested)\n",
    "hist_hybrid = hybrid_model.fit(\n",
    "    x=[train_seq, train_line_oh, train_total_oh], # 3 Inputs\n",
    "    y=train_y_oh,\n",
    "    epochs=5, batch_size=64,\n",
    "    validation_data=([test_seq, test_line_oh, test_total_oh], test_y_oh),\n",
    "    callbacks=[early_stop], verbose=1\n",
    ")\n",
    "dl_utils.plot_history(hist_hybrid, \"Hybrid Model\")\n",
    "# Evaluate\n",
    "pred_hybrid = tf.argmax(hybrid_model.predict([test_seq, test_line_oh, test_total_oh]), axis=1)\n",
    "dl_results_list.append(dl_utils.calculate_dl_results(test_y_enc, pred_hybrid, \"Hybrid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78180415-5f12-45d9-8a8d-f66177e670a7",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. The Solution: \"Tribrid\" Embedding Model\n",
    "To break the **80%** ceiling, we engineered a model that mimics how a human reads an abstract: we look at the text, but we also know if we are at the start or end of the paragraph.\n",
    "\n",
    "### Feature Engineering: Positional Embeddings\n",
    "We created two new features to act as \"map coordinates\" for the model:\n",
    "1. **Line Number:** One-hot encoded index of the sentence (capped at 15).\n",
    "2. **Total Lines:** One-hot encoded length of the abstract (capped at 20).\n",
    "\n",
    "### The Hybrid Architecture\n",
    "We designed a **Tribrid Model** that accepts three simultaneous inputs:\n",
    "1. **Text Sequence** (processed by LSTM) → Content\n",
    "2. **Line Number** (Dense Layer) → Position\n",
    "3. **Total Lines** (Dense Layer) → Context\n",
    "\n",
    "### Final Results\n",
    "* **Performance**: Massive jump to ~88.1% Accuracy.\n",
    "* **Conclusion**: Validated the hypothesis. By explicitly telling the model where the sentence is, it can easily distinguish \"Background\" (Sentence 0) from \"Objective\" (Sentence 2) even if the vocabulary is similar.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde00ad-ca76-4bd2-912c-e6115d6802ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL COMPARISON\n",
    "dl_results_df = pd.DataFrame(dl_results_list)\n",
    "all_results = pd.concat([ml_results, dl_results_df], ignore_index=True)\n",
    "\n",
    "print(\"\\n--- FINAL LEADERBOARD ---\")\n",
    "print(all_results.sort_values(by=\"F1-Score (Weighted)\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864ca30-f92e-4620-b130-6e1088f0d2b8",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Project Summary & Conclusions\n",
    "**1.** The Challenge\n",
    "Classify medical abstract sentences into 5 roles (Background, Objective, Methods, Results, Conclusions). The core difficulty was the linguistic similarity between \"Background\" and \"Objective.\"\n",
    "**2.** The Modeling Journey\n",
    "* ML Baseline (77.3%): Proved that specific keywords (TF-IDF) are strong predictors.\n",
    "* Deep Learning Baseline (80.0%): LSTMs improved performance by capturing sequence, but hit a ceiling due to lack of structural context.\n",
    "* Hybrid \"Tribrid\" Model (88.1%): The breakthrough came from fusing text embeddings with positional features (Line Number + Total Lines).\n",
    "**3.** Key Takeaway\n",
    "In structured document classification, domain-specific feature engineering (positional embeddings) is often more impactful than simply increasing the complexity of the neural network. We outperformed the baseline by ~11% not by making the LSTM deeper, but by giving it better data.\n",
    "**4.** Future Work\n",
    "To push beyond 88%, the next step is Transfer Learning (BioBERT): replacing our custom LSTM embeddings with a Transformer model pre-trained specifically on biomedical texts.\n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
